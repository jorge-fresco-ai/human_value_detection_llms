# ğŸ§  Human Value Detection with LLMs (ValueEval)

Este proyecto tiene como objetivo detectar valores humanos en textos mediante el uso de modelos de lenguaje grandes (LLMs), en el contexto de las tareas **ValueEval @ SemEval 2023** y **CLEF 2024 (TouchÃ© Lab)**. Se basa en la teorÃ­a de Schwartz, que categoriza 19 valores humanos en 4 dimensiones principales.

## ğŸ“Œ Objetivo General

Desarrollar y evaluar estrategias basadas en LLMs para identificar valores humanos y su polaridad (fomentado o restringido) en frases, empleando enfoques de Zero-Shot, Few-Shot con varios ejemplos y documentaciÃ³n extra.

## ğŸ¯ Objetivos EspecÃ­ficos

- Clasificar frases segÃºn los valores humanos presentes (basado en la teorÃ­a de Schwartz).
- Determinar la polaridad: si el valor estÃ¡ siendo promovido o restringido.
- Comparar el rendimiento de diferentes LLMs en Zero-Shot, Few-Shot y DocumentaciÃ³n Extra.
- Analizar errores, sesgos y efectividad prÃ¡ctica.

## ğŸ“ Estructura del Repositorio

```
â”œâ”€â”€ DATA/                    # Datasets anotados (train, val, test)
â”‚   â”œâ”€â”€ data_human_value/        # Datasets finales transformados y trabajados
â”‚   â”œâ”€â”€ prompts/                 # Prompts diseÃ±ados para cada enfoque
â”œâ”€â”€ CÃ“DIGO/                  # CÃ³digo relacionado con el TFG
â”‚   â”œâ”€â”€ models/                  # Configs o checkpoints de modelos usados
â”‚   â”œâ”€â”€ scripts/                 # Scripts de inferencia y evaluaciÃ³n
â”‚   â”œâ”€â”€ notebooks/               # Experimentos exploratorios y EDA
â”‚   â”œâ”€â”€ results/                 # Resultados (predicciones, mÃ©tricas)
â”œâ”€â”€ DOCS/                    # DocumentaciÃ³n extendida
â””â”€â”€ README.md                # Este archivo
```

## ğŸ§  TaxonomÃ­a de Valores Humanos

Basado en la teorÃ­a de Schwartz, el proyecto trabaja con 19 valores humanos agrupados en 4 dimensiones:

| DimensiÃ³n             | Valores Ejemplo                          |
|----------------------|-------------------------------------------|
| Openness to Change   | Self-direction, Stimulation               |
| Self-enhancement     | Achievement, Power, Hedonism              |
| Conservation         | Security, Conformity, Tradition           |
| Self-transcendence   | Universalism, Benevolence, Humility       |

**Openness to Change**
- Self-Direction (Thought)
- Self-Direction (Action)
- Stimulation
- Hedonism

**Self-Enhancement**
- Achievement
- Power (Dominance)
- Power (Resources)
- Face

**Conservation**
- Security (Personal)
- Security (Societal)
- Tradition
- Conformity (Rules)
- Conformity (Interpersonal)
- Humility

**Self-Transcendence**
- Benevolence (Dependability)
- Benevolence (Caring)
- Universalism (Concern)
- Universalism (Nature)
- Universalism (Tolerance)

## ğŸ“¦ Dataset

- **Nombre**: ValueEval
- **Origen**: SemEval 2023 y CLEF 2024 (TouchÃ© Lab)
- **Idiomas**: EN, ES, FR, DE, IT, TR, HE, EL, NL, BG
- **Anotadores**: 70+ expertos en valores humanos
- **TamaÃ±o**: 2648 textos segmentados en ~74,000 frases
- **Formato**: Frase + Etiquetas binarias por valor + Polaridad (attained/constrained)

## ğŸ§ª MÃ©todos Usados

### 1. **Zero-Shot Learning**
- Sin ejemplos, usando descripciones explÃ­citas del valor.
- Ejemplo de modelo: GPT-4 con instrucciones tipo CoT (Chain of Thought).

### 2. **Few-Shot Learning**
- Con 1, 3, 5 y 10 ejemplos. TambiÃ©n versiÃ³n con definiciones + ejemplos.
- Modelos evaluados: ChatGPT, Claude, Gemini, LLaMA.

## âš™ï¸ Herramientas y Frameworks

- **LLMs**: OpenAI, Claude, LLaMA, Gemini, Mistral, Phi, etc.
- **Frameworks LLMs**: Ollama
- **Agentes**: LangGraph, LangChain, CrewAI
- **EvaluaciÃ³n**: scikit-learn, pandas
- **Entorno local**: Ollama, Docker, FastAPI

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

- **Macro F1-Score** (principal)
- PrecisiÃ³n, Recall, Matriz de ConfusiÃ³n
- AnÃ¡lisis por clase (valor) y polaridad
- Tiempo de inferencia en RAG vs otros mÃ©todos

## â“ Preguntas Clave

- Â¿QuÃ© LLM es mÃ¡s preciso y generalizable en esta tarea?
- Â¿CuÃ¡l es el impacto del nÃºmero de ejemplos en Few-Shot?
- Â¿CÃ³mo mitigar sesgos hacia valores frecuentes?
- Â¿QuÃ© diseÃ±o de prompt optimiza el rendimiento?

## ğŸ“Œ Resultados Relevantes

- GPT-4 obtuvo mejor rendimiento en polaridad (subtarea 2).
- Modelos multilingÃ¼es como XLM-RoBERTa destacaron en detecciÃ³n (subtarea 1).
- El uso de prompts contextuales mejorÃ³ mÃ©tricas frente a los directos.

## ğŸ“š Referencias

- ValueEval @ CLEF 2024: [Enlace](https://touche.webis.de)
- ValueEval @ SemEval 2023: [Enlace](https://semeval.github.io/)
- Schwartz Value Theory: [Wikipedia](https://en.wikipedia.org/wiki/Theory_of_Basic_Human_Values)

## ğŸ§  CrÃ©ditos

Desarrollado como parte de un Trabajo Fin de Grado sobre la DetecciÃ³n de Valores Humanos mediante LLMs. Basado en los trabajos de TouchÃ© 2024 y SemEval 2023, junto con fundamentos teÃ³ricos en PsicologÃ­a Social.

----
